---
title: "PLS_DA"
author: "Grupo 7-Proyecto"
date: "2024-05-09"
output: html_document
---
##MODELO PLS_DA

```{r,warning=FALSE,error=FALSE,include=FALSE}
library(knitr)
library(dplyr)
library(ggplot2)
library(readxl)
library(pls)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# if (!requireNamespace("BiocManager", quietly = TRUE))
#   install.packages("BiocManager")
# BiocManager::install("ropls")
library(ropls)
library(tidyr)
library(tibble) 


# https://bioconductor.org/packages/release/bioc/vignettes/ropls/inst/doc/ropls-vignette.html
```

### DATA READING AND PREPARATION (1st eval)

We will load all the data necessary for the development of this model and show how they appear in order to get an idea. 

```{r}
df <- read_excel("df_definitivo.xlsx", sheet = "datos")

df <- df %>%
  select(-primera_eval_num, -Mejor_resp_num)
head(df)
```

Next we select the first response variable on the one hand, factored since it is numerical and we need it as a categorical variable. On the other hand, the first 53 columns (not including the patient id) will be the ones we will use to try to predict it.

```{r}
Y <- df$pri_eval_num_ok  
X <- df[, 2:53] 

X <-as.matrix(X)
Y <-as.matrix(as.factor(Y))

factor(Y)

head(data.frame(X))
```

In the following step, we do the same for the second response variable, also treating it as a factor, where we will separate the data necessary for its prediction in another dataframe. In this way, we will not mix predictor variables with each variable to be predicted, eliminating the unnecessary.

```{r}
df2 <- df %>%
  select(-primera_eval)
Y2 <- df2$mejor_resp_num_ok  
X2 <- df2[, 2:64] 

X2 <-as.matrix(X2)
Y2 <-as.matrix(Y)

factor(Y2)
head(data.frame(X2))
```

### MODEL ESTIMATION (1st eval)

Let us now turn to component selection to predict the first response variable by looking at the cumulative measures of the cumulative $R^2$ and $Q^2$. 

```{r}
myplsda = opls(x = X, y = Y, predI = NA, crossvalI = 10, 
               scaleC = "standard", fig.pdfC = "none")
maxNC = 10 
myplsda = opls(x = X, y = Y, predI = maxNC, crossvalI = 10, 
              scaleC = "standard", fig.pdfC = "none")
plot(1:maxNC, myplsda@modelDF$`R2Y(cum)`, type = "o", pch = 16, col = "blue3",
     lwd = 2, xlab = "Components", ylab = "", ylim = c(0.4,0.8),
     main = "PLS-DA model: 1st eval")
lines(1:maxNC, myplsda@modelDF$`Q2(cum)`, type = "o", pch = 16, col = "red3",
      lwd = 2)
abline(h = 0.5, col = "red3", lty = 2)
legend("bottomleft", c("R2Y", "Q2"), lwd = 2, 
       col = c("blue3", "red3"), bty = "n")

```

As can be seen, no value of $Q^2$ appears in the graph. If we look at the values printed before the graph, we can see that the value of this cumulative measure is negative. Moreover, in the first message the package itself warns us that not even the first component is significant. 

The fact that the $Q^2$ is negative may indicate that the variance explained by the components of the model is not correctly captured and therefore the model does not have the necessary information to describe the data. 

However, we will try selecting three components, to see the values that are far away from the model and those with the highest error. If any of these turn out to be very significant, we can remove them and see if they improve the model.

```{r}
myplsda = opls(x = X, y = Y, predI = 3, crossvalI = 2, 
               permI = 20, scaleC = "standard")
```

- In the graph on the top left, we see the value in each selected component of $R^2$ and $Q^2$. We have already discussed these measures cumulatively and it only remains to add how individually at least the first component does not have such a low value of $Q^2$. As for the $R^2$ values, they could be acceptable if they were accompanied by the second measure. 

- The second graph detects the outliers, we will study this case in the validation of the model, but already at a glance we see how for the $Q^2$ almost more than half of the values exceed the corresponding confidence line. 


- In the third graph (bottom left), we see the representation of the patients in space and their orthogonal distances according to the scores. We can already see how one patient (11) is far away from the rest. 

- Finally we see the graph of the component scores, again with very little explanation of the data (only 28%) being explained by the first two components.

### MODEL VALIDATION (1st eval)
#### HOTTELING $T^2$ 

Let's move on to the section where a diagnosis of the anomalies in the model is made with respect to the patients.

```{r fig.width=5, fig.height=5}
misScores = myplsda@scoreMN
varT = apply(misScores, 2, var)
miT2 = colSums(t(misScores**2) / varT)
N = nrow(X)
A = 3
F95 = A*(N**2 - 1)/(N*(N - A)) * qf(0.95, A, N-A);
F99 = A*(N**2 - 1)/(N*(N - A)) * qf(0.99, A, N-A);

plot(1:length(miT2), miT2, type = "l", xlab = "Patients", ylab = "T2",
     main = "PLS: T2-Hotelling")
abline(h = F95, col = "orange", lty = 2, lwd = 2)
abline(h = F99, col = "red3", lty = 2, lwd = 2)


```


```{r}
which(miT2>F95)
which(miT2>F99)

```
It can be seen at a glance that two of the values are very far away from the model, especially patient no. 11, which exceeds the second confidence line of 99%. We will see through the residual sum of squares, next point, if again the same patients are the furthest away from the model or if more appear. Based on this, we will have to take the decision to perhaps eliminate these patients, even if that puts us in a bind, due to the very low amount of data available to us. 

#### SCR 

```{r}
myT = myplsda@scoreMN
myP = myplsda@loadingMN
myE = scale(X) - myT%*%t(myP) 
mySCR = rowSums(myE^2)   # SPE 
plot(1:length(mySCR), mySCR, type = "l", main = "SCR", 
     xlab = "Patients")

g = var(mySCR)/(2*mean(mySCR))
h = (2*mean(mySCR)^2)/var(mySCR)
chi2lim = g*qchisq(0.95, df = h)
abline(h = chi2lim, col = "orange", lty = 2)
chi2lim99 = g*qchisq(0.99, df = h)
abline(h = chi2lim99, col = "red3", lty = 2)

```


```{r}
which((mySCR>chi2lim))
```
Surprisingly, we do observe three observations further away from the model in terms of SCR, but what is striking is that patient no. 11 is anomalous in the first case and within the "normal" distances in terms of this measure. Patient no. 17 also do not appear here, nevertheless 4, 9 and 29 which were not even considered before. 

Due to the low amount of data we have, we can hardly afford to eliminate data. In addition, the distance by which it exceeded the second confidence limit in Hotelling's $T^2$ was not so great and the fact that this distant patient does not appear in this second measure may allow us to keep the information from that patient to create the model. The others did not exceed the second confidence limit in the SCR and for the same reason we will keep them. 

For all these reasons, we will keep all the patients and we will interpret the results in the prediction of this first variable. 

### MODEL INTERPRETATION (1st eval)

```{r interprCan, message=FALSE}
plot(x = myplsda, typeVc = "x-score",
     parCexN = 0.8, parCompVi = c(1, 2), parPaletteVc = NA,
     parTitleL = TRUE, parCexMetricN = NA)
plot(x = myplsda, typeVc = "x-score",
     parCexN = 0.8, parCompVi = c(1, 3), parPaletteVc = NA,
     parTitleL = TRUE, parCexMetricN = NA)

```
Firstly, the low explanation of the components of the whole model. Only these 3 components account for 36% of the total explanation of the information in the data. 

Next, we can add the interpretation of these first graphs, where the groupings according to patient scores can be observed. In the first graph we can see how the first response 3, in which the disease progresses, its patients are the least overlapped and perhaps the model could predict this group better because of this. This overlap is hardly observed in components 3 and 1 either.
On the other hand, the difference is not so great for the first two groups as they appear more overlapped in both graphs, which may indicate that with these 3 components we cannot explain their differences so much and the model in these cases may find it more difficult to make the prediction. 

For a better interpretation, it will be better to observe the loadings of each variable indicating the groups to see which ones appear closer to which groups and further away diagonally, to see also the negative influence. 

```{r interprCan2, message=FALSE, fig.height=7, fig.width=5}
plot(x = myplsda, typeVc = "xy-weight",
     parCexN = 0.7, parCompVi = c(1, 2), parPaletteVc = NA, 
     parTitleL = TRUE, parCexMetricN = NA)
```

With this graph we can see how in the case of patients in the first group, where there is a partial response to the disease with treatment, age has a particular influence, as does tumour histology, i.e. its composition and c-reactive protein, which provides information about inflammation in the body. 

In general, it can be observed that most variables have a very low loading and are therefore clustered in large numbers in the centre of the graph. They will have little influence on the classification into any group. 

However, we do see clusters of variables around the second and third groups. 

In the second group, measurements taken after the first cycle seem to have a positive influence on a pseudo-response and stable disease, while heart disease, lymphocytes and the patient's height have a negative influence on this classification. 

As for the group in which the disease progresses even with treatment, which we saw in the scores, the closest variables are total cholesterol, the cancer inflammation index and the PNI directly. Conversely, almost all of those mentioned as direct would be in the second group, as these two are almost inversely related. This is surprising as one would expect more of this reversal of correlation between the more distant groups, but instead this occurs between groups 2 and 3. 


### Error measurements in PLS-DA (1st eval)


```{r, message=FALSE, warning=FALSE}
mypred = predict(myplsda)
library(caret)
confusionMatrix(factor(mypred), factor(Y), positive = "3")
```
Despite what we have been saying that perhaps the model did not capture all the variability, the predictions have turned out to be fairly accurate with an accuracy of 95%. However, these results must firstly be broken down, which we will do below, and secondly, it must be borne in mind that this was not done during a cross-validation process as it was not possible given the small size of the data. 

We found a kappa index of over 90%, which leads to an indication of the model's lag with respect to random predictions. This gives some confidence to the observed accuracy in the prediction of almost 95%, an excellent figure a priori. 

If we go class by class studying the results, we see that in almost all of them it performs an almost perfect classification, perhaps in the third class (disease progression) is the one where it reaches the lowest true positives, but this may also be due to the imbalance of the classes. A stratified kfold would allow us to cross-validate by removing the relevance of this unbalance and thus perhaps obtain better results in a way that is less influenced by the difference in observations per class. The fact that it occurs for this class is not a very good sign as it is the class that we would perhaps be most interested in knowing about because of the severity of the classification. Still, it is a very good result considering the overall accuracy away from randomness, i.e. where the model knows the reason for the classification. This has already been mentioned to a large extent in the interpretation of the model.

### MODEL ESTIMATION (best response)

Let's move on to the prediction of the second response variable with the creation of its model for it.

```{r}
maxNC = 10 
myplsda = opls(x = X2, y = Y2, predI = maxNC, crossvalI = 2, 
              scaleC = "standard", fig.pdfC = "none")
plot(1:maxNC, myplsda@modelDF$`R2Y(cum)`, type = "o", pch = 16, col = "blue3",
     lwd = 2, xlab = "Components", ylab = "",
     main = "PLS-DA model: 1st eval")
lines(1:maxNC, myplsda@modelDF$`Q2(cum)`, type = "o", pch = 16, col = "red3",
      lwd = 2)
abline(h = 0.5, col = "red3", lty = 2)
legend("bottomleft", c("R2Y", "Q2"), lwd = 2, 
       col = c("blue3", "red3"), bty = "n")

```
In this case it seems that the $Q^2$ parameter is not observed either, as it does get a somewhat higher value than in the first case, but again, it is negative and is not appreciated. It will contain very little information about the variability of the data, even less than randomly.  It seems that the estimation of components is again unclear. However, we decide to select 4, as they show a high value for the $R^2$, almost 75%. Again, we will not be able to select the number of components taking into account the value of $Q^2$, as it is very bad. 

Let us therefore select 4 components to create this PLS-DA model for the prediction of the best patient response to treatment with this drug.


```{r}
myplsda2 = opls(x = X, y = Y, predI = 4, crossvalI = 2, 
               permI = 20, scaleC = "standard")
```

As in the first case, all these graphs will be discussed in more detail below, one by one.  

### MODEL VALIDATION 
#### HORELLING $T^2$


```{r fig.width=5, fig.height=5}
misScores = myplsda2@scoreMN
varT = apply(misScores, 2, var)
miT2 = colSums(t(misScores**2) / varT)
N = nrow(X)
A = 4
F95 = A*(N**2 - 1)/(N*(N - A)) * qf(0.95, A, N-A);
F99 = A*(N**2 - 1)/(N*(N - A)) * qf(0.99, A, N-A);

plot(1:length(miT2), miT2, type = "l", xlab = "Patients", ylab = "T2",
     main = "PLS: T2-Hotelling",ylim=c(0,20))
abline(h = F95, col = "orange", lty = 2, lwd = 2)
abline(h = F99, col = "red3", lty = 2, lwd = 2)

```

```{r}
which(miT2>F95)
which(miT2>F99)
```
In this case, some observations exceed the first 95% confidence line, but in no case do they exceed the second 99% confidence line. This indicates the existence of anomalies for these two patients, but they are not so serious that the observations have to be removed from the dataframe with which the model is created. However, it will be necessary to look at the second measure in order to appreciate the distance to the model and thus be able to make the relevant decisions, or not. 

#### SCR
```{r}
myT = myplsda2@scoreMN
myP = myplsda2@loadingMN
myE = scale(X) - myT%*%t(myP) 
mySCR = rowSums(myE^2)   # SPE 
plot(1:length(mySCR), mySCR, type = "l", main = "SCR", 
     xlab = "Patients")

g = var(mySCR)/(2*mean(mySCR))
h = (2*mean(mySCR)^2)/var(mySCR)
chi2lim = g*qchisq(0.95, df = h)
abline(h = chi2lim, col = "orange", lty = 2)
chi2lim99 = g*qchisq(0.99, df = h)
abline(h = chi2lim99, col = "red3", lty = 2)

```

```{r}
which(mySCR>chi2lim)
which(mySCR>chi2lim99)

```

As mentioned above, even if some values exceed the first confidence range of the model's remoteness, it is not necessary to eliminate these associated patients. This is primarily due to the small amount of data available to train the model. This makes the removal of some of the values more relevant later in the model building. Therefore, this process will not be carried out as it is not necessary. If we had a lot of data, perhaps we could consider this idea, but for the problem we are dealing with it is not even feasible because of what we have explained.  


Let us move on to the interpretation of the model that tries to predict the second response variable. 

### MODEL INTERPRETATION AFTER ELIMINATION (best response)

```{r}
plot(x = myplsda2, typeVc = "x-score",
     parCexN = 0.8, parCompVi = c(1, 2), parPaletteVc = NA,
     parTitleL = TRUE, parCexMetricN = NA)
plot(x = myplsda2, typeVc = "x-score",
     parCexN = 0.8, parCompVi = c(3, 4), parPaletteVc = NA,
     parTitleL = TRUE, parCexMetricN = NA)

```
If we look at the latent variable scores of the model that show the clustering of the observations, first of all we can see that of the 45% explained, the 9% provided by the 4th component clusters the data more and produces the overlap with the 3rd component that did not exist in the previous model. 

In addition, we can observe in the rest of the component practically what was mentioned before, the greater distinction with respect to group 3, which is more distant in the spatial representation, while the other two groups produce more overlaps. This, as we have said previously, can be translated into greater complexity in differentiating groups 1 and 2 with respect to the third. 
```{r}
plot(x = myplsda2, typeVc = "xy-weight",
     parCexN = 0.7, parCompVi = c(1, 2), parPaletteVc = NA, 
     parTitleL = TRUE, parCexMetricN = NA)

```
Again, when looking at the variable loadings plotted next to where the groups appear, we see the large central clustering that occurs in most variables.
Variables related to response 1 (partial treatment response) such as age, histology and c-reactive protein are again observed.

The groupings of the variables for each group are virtually identical as in the first case, this makes sense since the first assessment actually measures the patients' tumour response to the drug, and the best response which is our second predictor variable measures which of all the assessments has been the best. This is why in many cases these variables coincide and the influence of the explanatory variables on them is identical. 
This would explain why there is hardly any difference in the PLS-DA models of both variables, both in the estimation of the components and in the outliers and with a greater distance to the predictive models. 


### ERROR MEASUREMENTS IN PLS-DA AFTER ELIMINATINO (best response) 

Let us finally look at the associated confusion matrix to see the failures broken down by category. This will allow us to know what kind of error we are making, as it is a bigger error to predict to someone that the treatment will give a very good best response and that the response is negative, than to predict a response where the tumour grows and yet partial disease progression occurs so that the tumour shrinks.


```{r prediDAtrain, message=FALSE, warning=FALSE}
mypred = predict(myplsda2)
library(caret)
confusionMatrix(factor(mypred), factor(Y), positive = "3")
```

The results in the confusion matrix are identical to those in the first variable, because, as already mentioned, the two response variables are very similar and the second one is in many cases taken from the first evaluation. Therefore, the explanation of the previous results is valid for us. 


### LOO FOR BOTH MODELS (LEAFT ONE OUT)

Next, in view of the doubt in the accuracy of the results generated by the models, we are going to carry out a LOO process to obtain the real accuracy of them as we do not know the test data since they will not be included as part of the training of the models. As we have mentioned, this was the case in the previous cases, which is why they had such high values in terms of accuracy and kappa index. 

```{r}
loo_evaluation <- function(nComp, X, y) {
  predictions <- numeric(length(y))
  for (i in 1:length(y)) {
    # Dejar una muestra fuera
    X_loo <- X[-i, , drop = FALSE]
    y_loo <- factor(y[-i])
    
    model_loo <- plsda(X_loo, y_loo, ncomp = nComp)
    
    # Predecir la clase de la muestra eliminada
    predictions[i] <- predict(model_loo, newdata = X[i, , drop = FALSE])
  }
  return(predictions)
}

# Obtener predicciones LOO
loo_predictions <- loo_evaluation(2, X, Y)

# Crear la matriz de confusión
confusionMatrix(factor(loo_predictions), factor(Y))
```
The results are no longer surprising, given the low variability explained by the model components. The accuracy is lower than the random classification, which is underlined by the negative kappa index. This would not be a good model for classification in our problem. Only perhaps for class 3, where it manages to overcome a random accuracy, we could consider using it as it has a balanced accuracy of almost 70%. However, the robustness is not very high and we could not rely much on it to predict the first variable. 


```{r}
loo_predictions=loo_evaluation(4,X2,Y2)
confusionMatrix(factor(loo_predictions),factor(Y2))
```
But this model has an even more negative kappa and a lower accuracy. As we have said, the class that is perhaps the best predicted is class 3 because it is the furthest away, as we have seen in the vector space. However, it is not very reliable since it is a minimal context where the aim is to reduce the prediction of false positives (the reduction of the disease being positive) given the serious consequences of not doing so.

